{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac0a317a-19a0-4990-b2ea-3dec0be7b568",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark duo fuxi this is cool mike\", 1.0),\n",
    "    (1, \"b d f i hate mike hate data \", 0.0),\n",
    "    (2, \"spark i love spark ddd example hadoop\", 1.0),\n",
    "    (3, \"i love coding and ml\", 0.0),\n",
    "   (4, \"i want to move fast\", 1.0), \n",
    "   (5, \"Mike like student to ask question\", 1.0),\n",
    "   (6, \"Mike hate people to ask hwo to import spark URL\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f422093-1618-4bb8-aeb0-da55bf5825f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>text</th><th>label</th></tr></thead><tbody><tr><td>0</td><td>a b c d e spark duo fuxi this is cool mike</td><td>1.0</td></tr><tr><td>1</td><td>b d f i hate mike hate data </td><td>0.0</td></tr><tr><td>2</td><td>spark i love spark ddd example hadoop</td><td>1.0</td></tr><tr><td>3</td><td>i love coding and ml</td><td>0.0</td></tr><tr><td>4</td><td>i want to move fast</td><td>1.0</td></tr><tr><td>5</td><td>Mike like student to ask question</td><td>1.0</td></tr><tr><td>6</td><td>Mike hate people to ask hwo to import spark URL</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         "a b c d e spark duo fuxi this is cool mike",
         1.0
        ],
        [
         1,
         "b d f i hate mike hate data ",
         0.0
        ],
        [
         2,
         "spark i love spark ddd example hadoop",
         1.0
        ],
        [
         3,
         "i love coding and ml",
         0.0
        ],
        [
         4,
         "i want to move fast",
         1.0
        ],
        [
         5,
         "Mike like student to ask question",
         1.0
        ],
        [
         6,
         "Mike hate people to ask hwo to import spark URL",
         0.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "text",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "label",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197ef349-c4e3-4202-99cc-486de92da1c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "# build a pipeline \n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "# pipeline.saveAs(\"location\")\n",
    "# pipeline.load(\"location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab0c244-0b01-4982-8601-fa4bff252278",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preprocess_model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b99e82d-2bab-463e-a850-f393be0d4543",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(training)\n",
    "# model.saveAs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03d92cd-9d72-475f-a3f7-6d11a03e6f11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "698e30da-a9ee-41b1-a2b3-5bece8ffa652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>text</th></tr></thead><tbody><tr><td>4</td><td>spark i j k</td></tr><tr><td>5</td><td>l m n</td></tr><tr><td>6</td><td>spark hadoop spark</td></tr><tr><td>7</td><td>apache hadoop</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4,
         "spark i j k"
        ],
        [
         5,
         "l m n"
        ],
        [
         6,
         "spark hadoop spark"
        ],
        [
         7,
         "apache hadoop"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "text",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81733df4-a4d1-4e76-a32e-daaed11d066b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7066f91c-7fd8-4a11-a625-a455246968d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hash_results = preprocess_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4735741e-1046-48d9-8b42-cab3e82caab6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>text</th><th>probability</th><th>prediction</th></tr></thead><tbody><tr><td>4</td><td>spark i j k</td><td>Map(vectorType -> dense, length -> 2, values -> List(0.15933224223332482, 0.8406677577666752))</td><td>1.0</td></tr><tr><td>5</td><td>l m n</td><td>Map(vectorType -> dense, length -> 2, values -> List(0.17275995885020773, 0.8272400411497922))</td><td>1.0</td></tr><tr><td>6</td><td>spark hadoop spark</td><td>Map(vectorType -> dense, length -> 2, values -> List(0.01575278226492635, 0.9842472177350736))</td><td>1.0</td></tr><tr><td>7</td><td>apache hadoop</td><td>Map(vectorType -> dense, length -> 2, values -> List(0.04224348512947865, 0.9577565148705214))</td><td>1.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4,
         "spark i j k",
         {
          "length": 2,
          "values": [
           0.15933224223332482,
           0.8406677577666752
          ],
          "vectorType": "dense"
         },
         1.0
        ],
        [
         5,
         "l m n",
         {
          "length": 2,
          "values": [
           0.17275995885020773,
           0.8272400411497922
          ],
          "vectorType": "dense"
         },
         1.0
        ],
        [
         6,
         "spark hadoop spark",
         {
          "length": 2,
          "values": [
           0.01575278226492635,
           0.9842472177350736
          ],
          "vectorType": "dense"
         },
         1.0
        ],
        [
         7,
         "apache hadoop",
         {
          "length": 2,
          "values": [
           0.04224348512947865,
           0.9577565148705214
          ],
          "vectorType": "dense"
         },
         1.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "text",
         "type": "\"string\""
        },
        {
         "metadata": "{\"ml_attr\":{\"num_attrs\":2}}",
         "name": "probability",
         "type": "{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}"
        },
        {
         "metadata": "{\"ml_attr\":{\"type\":\"nominal\",\"num_vals\":2}}",
         "name": "prediction",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d5cfa5e-bda8-4754-b359-635140aaab34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.15933224223332482,0.8406677577666752], prediction=1.000000\n(5, l m n) --> prob=[0.17275995885020773,0.8272400411497922], prediction=1.000000\n(6, spark hadoop spark) --> prob=[0.01575278226492635,0.9842472177350736], prediction=1.000000\n(7, apache hadoop) --> prob=[0.04224348512947865,0.9577565148705214], prediction=1.000000\n"
     ]
    }
   ],
   "source": [
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8cfd922-ac49-48e7-bf5f-80e9a0e36211",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5811d164-2ed7-4fc6-a343-41acf5cca267",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n| id|category|categoryIndex|\n+---+--------+-------------+\n|  0|       a|          0.0|\n|  1|       b|          2.0|\n|  2|       c|          1.0|\n|  3|       a|          0.0|\n|  4|       a|          0.0|\n|  5|       c|          1.0|\n+---+--------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    " \n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a\"),\n",
    "    (1, \"b\"),\n",
    "    (2, \"c\"),\n",
    "    (3, \"a\"),\n",
    "    (4, \"a\"),\n",
    "    (5, \"c\")\n",
    "], [\"id\", \"category\"])\n",
    " \n",
    "stringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "indexed.show()\n",
    " \n",
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pipeline practice 01",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
