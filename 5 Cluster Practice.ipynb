{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6679af76-15a4-48d2-b4d7-f438f4123081",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n| id| lat|  long|\n+---+----+------+\n|  0|33.3| -17.5|\n|  1|40.4| -20.5|\n|  2|28.0| -23.9|\n|  3|29.5| -19.0|\n|  4|32.8|-18.84|\n+---+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[0, 33.3, -17.5],\n",
    "                              [1, 40.4, -20.5],\n",
    "                              [2, 28., -23.9],\n",
    "                              [3, 29.5, -19.0],\n",
    "                              [4, 32.8, -18.84]\n",
    "                             ],\n",
    "                              [\"id\",\"lat\", \"long\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be4a970b-c010-4716-9f68-5a5aaca44ad9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+\n| id| lat|  long|     features|\n+---+----+------+-------------+\n|  0|33.3| -17.5| [33.3,-17.5]|\n|  1|40.4| -20.5| [40.4,-20.5]|\n|  2|28.0| -23.9| [28.0,-23.9]|\n|  3|29.5| -19.0| [29.5,-19.0]|\n|  4|32.8|-18.84|[32.8,-18.84]|\n+---+----+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols=[\"lat\", \"long\"], outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(df)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f503bc31-93a5-4071-825f-f9a75200500d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=5, seed=1)  \n",
    "model = kmeans.fit(new_df.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc993ee5-9b6e-4826-9160-d407aaa6e193",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------+----------+\n| id| lat|  long|     features|prediction|\n+---+----+------+-------------+----------+\n|  0|33.3| -17.5| [33.3,-17.5]|         1|\n|  1|40.4| -20.5| [40.4,-20.5]|         2|\n|  2|28.0| -23.9| [28.0,-23.9]|         0|\n|  3|29.5| -19.0| [29.5,-19.0]|         3|\n|  4|32.8|-18.84|[32.8,-18.84]|         4|\n+---+----+------+-------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "transformed = model.transform(new_df)\n",
    "transformed.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bddabbea-5901-49ee-b3da-0816b208177d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 0.0\nCluster Centers: \n[ 28.  -23.9]\n[ 33.3 -17.5]\n[ 40.4 -20.5]\n[ 29.5 -19. ]\n[ 32.8  -18.84]\n"
     ]
    }
   ],
   "source": [
    "summary = model.summary\n",
    "wssse = summary.trainingCost\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "978397f3-77ff-40fa-92d6-6a41a198f2f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|age| name|\n+---+-----+\n|1.0|Alice|\n|2.0| Mike|\n|3.0| json|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data_list = [{'name': 'Alice', 'age': '1'}, \n",
    "     {'name': 'Mike', 'age': '2'},\n",
    "     {'name': 'json', 'age': '3'},\n",
    "    ]\n",
    " \n",
    "df_new = spark.createDataFrame(data_list)\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "changedTypedf = df_new.withColumn(\"age\", df_new[\"age\"].cast(DoubleType()))\n",
    "\n",
    "changedTypedf.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5 Cluster Practice",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
